{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNzqLU8zluNDAcZLs/ZgDjG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anuradha2504/LLaMa_Chatbot/blob/main/Chatbot_LLaMa_Anu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Install Depentencies - Installations**\n",
        "\n",
        "Essential libraries to be install:\n",
        "PyTorch: Serves as the backbone for deep learning operations.\n",
        "Accelerate: Optimizes PyTorch operations, especially on GPU."
      ],
      "metadata": {
        "id": "6bpWkmYXruJQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CABMIKatacbM",
        "outputId": "1412adc1-1ab2-413a-8b2d-fd393370d622"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.Loggin to huggingface**\n",
        "\n",
        "Take access on model - meta-llama/Llama-2-7b-chat-hf,\n",
        "Ensures we have the correct permissions to fetch the model.\n",
        "\n",
        "Gain access to the model on Hugging Face: Link.\n",
        "https://huggingface.co/meta-llama/Llama-2-7b-chat-\n",
        "\n",
        "Use the Hugging Face CLI to login and verify your authentication status."
      ],
      "metadata": {
        "id": "uypqE7eSs7ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "aeIcwDoMbx4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Load Model & Tokenizer**\n",
        "\n",
        "Here, we are preparing our session by loading both the Llama model and its associated tokenizer.\n",
        "\n",
        "The tokenizer will help in converting our text prompts into a format that the model can understand and process."
      ],
      "metadata": {
        "id": "qg12NzlrvA5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model = \"meta-llama/Llama-2-7b-chat-hf\" # meta-llama/Llama-2-7b-hf\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=True)"
      ],
      "metadata": {
        "id": "zXMloukqcTxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.Creating the Llama Pipeline**\n",
        "\n",
        "setting up a pipeline for text generation.\n",
        "\n",
        "This pipeline simplifies the process of feeding prompts to our model and receiving generated text as output."
      ],
      "metadata": {
        "id": "zWMYxIPWvME5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",  # LLM task\n",
        "    model=model,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "BY4Ak-AQcjuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Get Responses**\n",
        "\n",
        "With everything set up, let's see how Llama responds.\n",
        "\n",
        "Create chat helper function"
      ],
      "metadata": {
        "id": "1VZBfFsavsq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_llama_response(prompt: str) -> None:\n",
        "    \"\"\"\n",
        "    Generate a response from the Llama model.\n",
        "\n",
        "    Parameters:\n",
        "        prompt (str): The user's input/question for the model.\n",
        "\n",
        "    Returns:\n",
        "        None: Prints the model's response.\n",
        "    \"\"\"\n",
        "    sequences = llama_pipeline(\n",
        "        prompt,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        max_length=256,\n",
        "    )\n",
        "    print(\"Chatbot:\", sequences[0]['generated_text'])\n",
        "\n",
        "\n",
        "\n",
        "prompt = 'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n'\n",
        "get_llama_response(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LJQgxRlcw_q",
        "outputId": "e9cfcefa-8bd6-49e4-c7f8-66cbecc0d8ee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\n",
            "\n",
            "I have a bit of a thing for gritty, intense dramas with complex characters and thought-provoking themes.\n",
            "\n",
            "Do you have any recommendations?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Sample queries- Prompt**"
      ],
      "metadata": {
        "id": "FRtNoVd3v8P4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"I'm a programmer and Python is my favorite language because of it's simple syntax and variety of applications I can build with it.\\\n",
        "Based on that, what language should I learn next?\\\n",
        "Give me 5 recommendations\"\"\"\n",
        "get_llama_response(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DAINvQdn6D5",
        "outputId": "17fba118-472b-47f8-d261-e4db5fa0b05a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: I'm a programmer and Python is my favorite language because of it's simple syntax and variety of applications I can build with it.Based on that, what language should I learn next?Give me 5 recommendations and explain why you think they're good choices.\n",
            "\n",
            "Here are five programming languages you might consider learning next, based on your interest in Python:\n",
            "\n",
            "1. JavaScript: JavaScript is a popular language used for web development, and it's closely related to Python in terms of syntax and structure. It's also used in a wide range of applications, including mobile app development, game development, and server-side programming.\n",
            "\n",
            "Why it's a good choice: JavaScript is a versatile language that can be used for a variety of applications, and it's well-suited for beginners who are already familiar with Python's syntax.\n",
            "\n",
            "2. Java: Java is an object-oriented language that's widely used in enterprise software development, Android app development, and web development. It's known for its platform independence, which means that Java code can run on any device that has a Java Virtual Machine (JVM) installed.\n",
            "\n",
            "Why it's a good choice\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. More Queries**"
      ],
      "metadata": {
        "id": "XN-HysswwN0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'How to learn fast?\\n'\n",
        "get_llama_response(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM-bvGQNoDCg",
        "outputId": "8a2191a2-196c-4438-bb84-98dd26bfd9e4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: How to learn fast?\n",
            "\n",
            "There are many ways to learn quickly, but here are some strategies that can help:\n",
            "\n",
            "1. Set clear goals: Identify what you want to learn and set specific goals for yourself. This will help you stay motivated and focused.\n",
            "2. Break it down: Break down the task or subject you want to learn into smaller, manageable chunks. This will make it easier to digest and retain the information.\n",
            "3. Use visual aids: Visual aids such as diagrams, flowcharts, and pictures can help you understand complex concepts and retain information better.\n",
            "4. Practice consistently: Consistency is key to learning quickly. Set aside a specific time each day or week to practice and review what you have learned.\n",
            "5. Get enough sleep: Sleep plays an important role in memory consolidation and learning. Aim for 7-9 hours of sleep each night to help your brain process and retain information.\n",
            "6. Teach someone else: Teaching someone else what you have learned is a great way to reinforce your own knowledge and retain information.\n",
            "7. Use flashcards: Flashcards can help you memorize key terms and concepts quickly. Write\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'I love basketball. Do you have any recommendations of team sports I might like?\\n'\n",
        "get_llama_response(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOgocAjcoHoE",
        "outputId": "97b01a3f-6c93-4653-ee10-1f53efec0294"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: I love basketball. Do you have any recommendations of team sports I might like?\n",
            "\n",
            "Basketball is a great sport, but there are many other team sports that you might enjoy as well. Here are a few options to consider:\n",
            "\n",
            "1. Volleyball: Volleyball is a fun and fast-paced sport that involves hitting a ball over a net with your hands. It's a great workout and can be played both indoors and outdoors.\n",
            "2. Soccer: Soccer, or football as it's called in many parts of the world, is a popular team sport that involves kicking a ball into a goal. It's a great way to stay active and can be played on a variety of surfaces, including grass, turf, and indoor courts.\n",
            "3. Lacrosse: Lacrosse is a fast-paced sport that involves using a stick to catch, carry, and throw a ball into a goal. It's a great workout and can be played both indoors and outdoors.\n",
            "4. Field Hockey: Field hockey is a team sport that involves using a stick to hit a ball into a goal. It's a great workout\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'How to get rich?\\n'\n",
        "get_llama_response(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4qK4VBZoQnM",
        "outputId": "dd520d02-2339-4fbb-d19b-4181952656ef"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: How to get rich?\n",
            "\n",
            "There are many ways to become rich, but here are some general tips that can help you on your journey to wealth:\n",
            "\n",
            "1. Start by setting clear financial goals: What do you want to achieve? When do you want to achieve it? How much money do you need to make it happen? Write down your goals and make them specific, measurable, achievable, relevant, and time-bound (SMART).\n",
            "2. Live below your means: Spend less than you earn. Create a budget that accounts for all your expenses, and make sure you're not overspending. Cut back on unnecessary expenses like dining out, subscription services, and other luxuries.\n",
            "3. Invest wisely: Invest your money in assets that have a high potential for growth, such as stocks, real estate, or a small business. Do your research, diversify your portfolio, and avoid get-rich-quick schemes.\n",
            "4. Build multiple income streams: Don't rely on just one source of income. Create additional streams of income through freelancing, consulting, or starting a side hustle. This will help you build wealth faster and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Make it conversational**\n",
        "\n",
        "Let's create an interactive chat loop, where you can converse with the Llama model.\n",
        "\n",
        "Type your questions or comments, and see how the model responds!"
      ],
      "metadata": {
        "id": "5g0bgrRkw6sN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"bye\", \"quit\", \"exit\"]:\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "    get_llama_response(user_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bL4OxPwUobLu",
        "outputId": "d36c904f-c042-4d96-8a5f-ba4fe9fd50f8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: quit\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"You are a helpful, concise assistant.\"\n",
        "MAX_TOKENS = 512\n",
        "TEMPERATURE = 0.7\n",
        "TOP_P = 0.95\n",
        "\n",
        "def format_prompt(history, user_message):\n",
        "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
        "    for u, a in history:\n",
        "        if u:\n",
        "            messages.append({\"role\": \"user\", \"content\": u})\n",
        "        if a:\n",
        "            messages.append({\"role\": \"assistant\", \"content\": a})\n",
        "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    # Build chat prompt in LLaMA-2 format\n",
        "    prompt = \"\"\n",
        "    for m in messages:\n",
        "        if m[\"role\"] == \"system\":\n",
        "            prompt += f\"<s>[INST] <<SYS>>\\n{m['content']}\\n<</SYS>>\\n\"\n",
        "        elif m[\"role\"] == \"user\":\n",
        "            prompt += f\"{m['content']} [/INST] \"\n",
        "        elif m[\"role\"] == \"assistant\":\n",
        "            prompt += f\"{m['content']} </s><s>[INST] \"\n",
        "    return prompt\n",
        "\n",
        "def generate_chat_response(history, user_message):\n",
        "    prompt = format_prompt(history, user_message)\n",
        "    out = llama_pipeline(\n",
        "        prompt,\n",
        "        max_new_tokens=MAX_TOKENS,\n",
        "        do_sample=True,\n",
        "        temperature=TEMPERATURE,\n",
        "        top_p=TOP_P,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    reply = out[0][\"generated_text\"][len(prompt):]\n",
        "    return reply.strip()"
      ],
      "metadata": {
        "id": "_j4VltYgrYRp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Create chatbot UI with gradio**"
      ],
      "metadata": {
        "id": "EY0Ngd3IxFP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# 🦙 LLaMA-2 7B Chatbot\")\n",
        "    chatbot = gr.Chatbot(height=400)\n",
        "    with gr.Row():\n",
        "        msg = gr.Textbox(placeholder=\"Type a message...\", scale=8)\n",
        "        clear = gr.Button(\"Clear\", scale=1)\n",
        "    temp = gr.Slider(0.0, 1.5, value=TEMPERATURE, step=0.05, label=\"Temperature\")\n",
        "    top_p = gr.Slider(0.1, 1.0, value=TOP_P, step=0.05, label=\"Top-p\")\n",
        "    max_new = gr.Slider(64, 1024, value=MAX_TOKENS, step=32, label=\"Max new tokens\")\n",
        "\n",
        "    def respond(user_message, chat_history, temperature, top_p, max_new_tokens):\n",
        "        global TEMPERATURE, TOP_P, MAX_TOKENS\n",
        "        TEMPERATURE = float(temperature)\n",
        "        TOP_P = float(top_p)\n",
        "        MAX_TOKENS = int(max_new_tokens)\n",
        "        bot_message = generate_chat_response(chat_history, user_message)\n",
        "        chat_history = chat_history + [(user_message, bot_message)]\n",
        "        return \"\", chat_history\n",
        "\n",
        "    msg.submit(respond, [msg, chatbot, temp, top_p, max_new], [msg, chatbot])\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "JyXwKsOgrolx",
        "outputId": "3cd3692b-4175-4856-94e1-20a1cb5e7163"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-167579297.py:5: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(height=400)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d3f5b2ff810d67be70.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d3f5b2ff810d67be70.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. To access from use below url**\n",
        "\n",
        "Running on public URL: https://d3f5b2ff810d67be70.gradio.live"
      ],
      "metadata": {
        "id": "1LDNpW9ZybQ2"
      }
    }
  ]
}